{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "this cell run at:  2023-05-12 10:07:24.171293\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "#print the cpu and gpu name\n",
    "# print(torch.cuda.get_device_name(0))\n",
    "\n",
    "# print(torch.version.cuda)\n",
    "print(\"this cell run at: \",datetime.datetime.now())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a847445f22bd45c5a24cf70727f20ba7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\matan.s\\Anaconda3b\\envs\\base2\\lib\\site-packages\\huggingface_hub\\file_download.py:123: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\matan.s\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40a3554ad122466eb759fb09ab516568",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2659920d819b49a7ae7d71c619cad20c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9998656511306763}]\n"
     ]
    }
   ],
   "source": [
    "classifier = pipeline('sentiment-analysis')\n",
    "res=classifier(\"I love you\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eeec404699754433a98d6259490a7985",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/762 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdee674ca5694f8bb701aa1de33ebbda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/353M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a63e4104f064d3c9b57b8561638b6e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6c2b82d91f74d5fada481edabd45567",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91835b00b4d54ff7acf5cb4f8f9eaeea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "c:\\Users\\matan.s\\Anaconda3b\\envs\\base2\\lib\\site-packages\\transformers\\generation_utils.py:1359: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 50 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'tell me about steve jobs as a whole.'}]\n"
     ]
    }
   ],
   "source": [
    "gen=pipeline('text-generation',model='distilgpt2')\n",
    "res=gen('tell me about steve jobs')\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': \"tell me about steve jobs. The only people to want to know for sure who was behind this... well maybe that’s why I've decided to join a union and not be a part of this project for another month. As I said before, the best place to start is to be interested in your views. Be honest, I have absolutely no interest in what you think in this project. The only people to want to know for sure who was behind this... well maybe that’\"}, {'generated_text': 'tell me about steve jobs. I told him on Twitter that his idea was worth it -- I don\\'t know for sure what they were thinking....\\n\\n\\nHere\\'s another example of a problem with free speech: A free speechist who claims that it\\'s \"an essential part of the job market,\" claims he could only find \"someplace\" for the job market in the midtown Manhattan city under the guise of a free speech-friendly government. A free speechist who believes that'}, {'generated_text': 'tell me about steve jobs.›\\nIn an interview with the Sun and other newspapers, she said she has tried trying to get the federal government to provide an opportunity for low-wage jobs while helping the disabled. She said she hopes she can help them work.\\nThis story has been updated with additional information and facts.\\n\\n\\n\\n\\n\\nRolando Lepp is CEO and CEO of the U.S. Chamber of Commerce.'}, {'generated_text': \"tell me about steve jobs, which I think should be better than that but the real danger is that this will make every white man feel less welcome.”\\n\\nThe very idea that the idea of black racism is not just to save the nation from a terrible racial disaster, but to make America more prosperous. This is especially true of the young people whose college educations and other education are failing. The notion that these young people are actually white is one that they didn't even know that\"}, {'generated_text': \"tell me about steve jobs, I thought for certain I could do better than that and I still wanted to see the rest of the cast of the cast and all the members. I thought of the cast and their relationships and my feelings for working with me in this job. My last act was going to be the role of an African American father from Brooklyn who just couldn't afford to give up. He's never been as happy with me as he is about me, but he's willing to support\"}]\n"
     ]
    }
   ],
   "source": [
    "print(gen('tell me about steve jobs',max_length=100,num_return_sequences=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM,AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9998656511306763}]\n"
     ]
    }
   ],
   "source": [
    "classifier = pipeline('sentiment-analysis',model='distilbert-base-uncased-finetuned-sst-2-english')\n",
    "res=classifier(\"I love you\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name='distilbert-base-uncased-finetuned-sst-2-english'\n",
    "model=AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer=AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 101, 1045, 2293, 2017,  102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]])}\n",
      "['i', 'love', 'you']\n",
      "[1045, 2293, 2017]\n",
      "i love you\n"
     ]
    }
   ],
   "source": [
    "sequence=\"I love you\"\n",
    "res=tokenizer(sequence,return_tensors='pt')\n",
    "print(res)\n",
    "tokens=tokenizer.tokenize(sequence)\n",
    "print(tokens)\n",
    "ids=tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(ids)\n",
    "decoded_string=tokenizer.decode(ids)\n",
    "print(decoded_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9998656511306763}, {'label': 'NEGATIVE', 'score': 0.9991129040718079}]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "X_train=['I love you','I hate you']\n",
    "y_train=[1,0]\n",
    "res=classifier(X_train)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 101, 1045, 2293, 2017,  102],\n",
      "        [ 101, 1045, 5223, 2017,  102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "batch=tokenizer(X_train,padding=True,truncation=True,return_tensors='pt')\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-4.2756,  4.6393],\n",
      "        [ 3.8724, -3.1543]])\n",
      "tensor([[1.3436e-04, 9.9987e-01],\n",
      "        [9.9911e-01, 8.8707e-04]])\n",
      "tensor([1, 0])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    logits=model(**batch).logits\n",
    "    print(logits)\n",
    "    print(F.softmax(logits,dim=1))\n",
    "    labels=torch.argmax(F.softmax(logits,dim=1),dim=1)\n",
    "    print(labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
